{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35ef743",
   "metadata": {},
   "source": [
    "## Imports and small functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143ce1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "import matplotlib.backends.backend_pdf\n",
    "from scipy.stats import zscore\n",
    "import pickle\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "def calculate_pvalues(df):\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            tmp = df[df[r].notnull() & df[c].notnull()]\n",
    "            pvalues[r][c] = round(pearsonr(tmp[r], tmp[c])[1], 4)\n",
    "    return pvalues\n",
    "\n",
    "def distance(x, y):\n",
    "    if x >= y:\n",
    "        result = x - y\n",
    "    else:\n",
    "        result = y - x\n",
    "    return result\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "random_colors = cm.rainbow(np.linspace(0,1,8))\n",
    "cwd = os.getcwd()\n",
    "notebook_path = os.path.join(cwd,'cohort_analysis_calcium_pupil.ipynb')\n",
    "somas = {'RGECO_GCamP_Batch2_five':[],'RGECO_GCamP_Batch2_three':['FOV4']}\n",
    "short_term_strategy = {'both':[], \n",
    "                       'T':['GCaMP_POm_Batch3_five_220622_Nrev',\n",
    "                            'RGECO_GCamP_Batch1_muji_221230_Nrev',\n",
    "                            'RGECO_GCamP_Batch2_three_221220_Edet',\n",
    "                            'RGECO_GCamP_Batch2_three_230108_Erev',\n",
    "                            'RGECO_GCamP_Batch3_muji_230104_Edis_PUPIL',\n",
    "                            'RGECO_GCamP_Batch3_muji_230104_Nrev_PUPIL',\n",
    "                            'RGECO_GCamP_Batch3_one_230103_Ndis',\n",
    "                            'RGECO_GCamP_Batch3_one_230103_Edis',\n",
    "                            'RGECO_GCamP_Batch3_three_230105_Nrev',\n",
    "                            'RGECO_GCamP_Batch3_three_230105_Nrev_PUPIL'],\n",
    "                       'A':['RGECO_GCamP_Batch1_four_220921_Edet',\n",
    "                            'RGECO_GCamP_Batch2_three_221209_Ndis']\n",
    "                      }\n",
    "\n",
    "short_term_strategy = ['GCaMP_POm_Batch3_five_Nrev_220622',\n",
    "                        'RGECO_GCamP_Batch1_muji_Erev_221230',\n",
    "                        'RGECO_GCamP_Batch2_three_Edet_221220',\n",
    "                        'RGECO_GCamP_Batch2_three_Erev_230108',\n",
    "                        'RGECO_GCamP_Batch3_muji_Edis_230104_PUPIL',\n",
    "                        'RGECO_GCamP_Batch3_muji_Nrev_230104_PUPIL',\n",
    "                        'RGECO_GCamP_Batch3_one_Ndis_230103',\n",
    "                        'RGECO_GCamP_Batch3_one_Edis_230103',\n",
    "                        'RGECO_GCamP_Batch3_three_Nrev_230105',\n",
    "                        'RGECO_GCamP_Batch3_three_Nrev_230105_PUPIL',\n",
    "                        'RGECO_GCamP_Batch1_four_Edet_220921',\n",
    "                        'RGECO_GCamP_Batch2_three_Ndet_221209']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posteriors(group_df, size, protocol, dtype):\n",
    "    if dtype == 'pupil':\n",
    "        outcome_label = 'Outcome'\n",
    "    elif dtype == 'calcium':\n",
    "        outcome_label = 'Final Outcome'\n",
    "    else:\n",
    "        print('invalid dtype')\n",
    "        \n",
    "    posteriors = {}\n",
    "    for block in range(1000):        \n",
    "        if block+size > group_df.shape[0]:\n",
    "            break\n",
    "        else:\n",
    "            block_df = group_df[block:block+size]\n",
    "    \n",
    "            posteriors[block] = {}\n",
    "            posteriors[block]['reward prob'] = {}\n",
    "            posteriors[block]['stimulus ratios'] = {}\n",
    "            posteriors[block]['response rate'] = {}\n",
    "            posteriors[block]['rare prob'] = {}\n",
    "\n",
    "            \n",
    "            if 'det' not in protocol:\n",
    "                Go_ID = block_df[(block_df[outcome_label].str.contains('hit')) | (block_df[outcome_label].str.contains('miss'))].Stimulus.iloc[0]\n",
    "                NoGo_ID = float(1) if Go_ID == float(2) else float(2)\n",
    "                Go = block_df[block_df.Stimulus == Go_ID]\n",
    "                NoGo = block_df[block_df.Stimulus == NoGo_ID]\n",
    "            else:\n",
    "                Go = block_df[block_df.Stimulus != float(3)]\n",
    "                NoGo = block_df[block_df.Stimulus == float(3)]\n",
    "                \n",
    "            rew_probs = {}\n",
    "            response_probs = {}\n",
    "            for m, modality in zip(['G','N'],[Go, NoGo]):\n",
    "                nFA = modality[modality[outcome_label].str.contains('FA')].shape[0]\n",
    "                nHIT = modality[modality[outcome_label].str.contains('hit')].shape[0]\n",
    "                ntype = modality.shape[0]\n",
    "                nrew = modality[modality.Water == float(1)].shape[0]\n",
    "                rew_prob = nrew/(nFA+nHIT) if nFA+nHIT > 2 else np.nan\n",
    "                rew_probs[m] = rew_prob\n",
    "                response_probs[m]  = (nFA+nHIT)/ntype\n",
    "            rare = Go[(Go.Water == float(0)) & (Go[outcome_label].str.contains('hit'))].shape[0] + NoGo[(NoGo.Water == float(1)) & (NoGo[outcome_label].str.contains('FA'))].shape[0]\n",
    "            frequent = Go[(Go.Water == float(1)) & (Go[outcome_label].str.contains('hit'))].shape[0] + NoGo[(NoGo.Water == float(0)) & (NoGo[outcome_label].str.contains('FA'))].shape[0]\n",
    "            if (rare > 2) & (frequent > 2):\n",
    "                posteriors[block]['rare prob'] = rare/frequent\n",
    "            else:\n",
    "                posteriors[block]['rare prob'] = np.nan\n",
    "            posteriors[block]['reward prob'] = rew_probs\n",
    "            posteriors[block]['response rate'] = response_probs\n",
    "            \n",
    "            ntrials = block_df.shape[0]\n",
    "            posteriors[block]['stimulus ratios'] = {'G':Go.shape[0]/ntrials,\n",
    "                                                    'N':NoGo.shape[0]/ntrials}\n",
    "    \n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb438e2",
   "metadata": {},
   "source": [
    "## Data uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df057fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_colors = {}\n",
    "cohort_epochs = {}\n",
    "for dtype in ['events','deltaF','pupil']:\n",
    "    cohort_dtype = {}\n",
    "    for root, dirs, files in os.walk(cwd):\n",
    "        for f in files:\n",
    "            if (dtype+'_epochs' in f) & ('Old version' not in root):\n",
    "                ID = root.split('\\\\')[-1]\n",
    "                ID_dtype = np.load(root+'\\\\'+f, allow_pickle=True).item()\n",
    "\n",
    "                for epoch, epoch_df in ID_dtype.items():\n",
    "                    if dtype not in cohort_epochs.keys():\n",
    "                        cohort_epochs[dtype] = {}\n",
    "                        \n",
    "                        if epoch not in cohort_epochs[dtype].keys():\n",
    "                            cohort_epochs[dtype][epoch] = {}\n",
    "                            cohort_epochs[dtype][epoch][ID] = epoch_df\n",
    "                        else:\n",
    "                            cohort_epochs[dtype][epoch][ID] = epoch_df\n",
    "                    else:\n",
    "                        if epoch not in cohort_epochs[dtype].keys():\n",
    "                            cohort_epochs[dtype][epoch] = {}\n",
    "                            cohort_epochs[dtype][epoch][ID] = epoch_df\n",
    "                        else:\n",
    "                            cohort_epochs[dtype][epoch][ID] = epoch_df\n",
    "                            \n",
    "for i, (mouse) in enumerate(cohort_epochs['deltaF']['all'].keys()):\n",
    "    ID_colors[mouse] = random_colors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_deltaF = {}\n",
    "cohort_correlations = {}\n",
    "cohort_pupil = {}\n",
    "cohort_events = {}\n",
    "cohort_params = {}\n",
    "for root, dirs, files in os.walk(cwd):\n",
    "    for f in files:\n",
    "        if ('processed_calcium.npy' in f) & ('Old version' not in root):\n",
    "            ID = root.split('\\\\')[-1]\n",
    "            ID_calcium = np.load(root+'\\\\'+f, allow_pickle=True).item()\n",
    "            ID_deltaF = ID_calcium['deltaF']\n",
    "            ID_correlations = ID_calcium['correlations']\n",
    "            ID_events = ID_calcium['events']\n",
    "            cohort_deltaF[ID] = ID_deltaF\n",
    "            cohort_correlations[ID] = ID_correlations\n",
    "            cohort_events[ID] = ID_events\n",
    "        if ('processed_pupil.csv' in f) & ('Old version' not in root):\n",
    "            ID = root.split('\\\\')[-1]\n",
    "            ID_pupil = pd.read_csv(root+'\\\\'+f, index_col=list(range(17)))\n",
    "            cohort_pupil[ID] = ID_pupil\n",
    "        if ('params.npy' in f) & ('Old version' not in root):\n",
    "            ID = root.split('\\\\')[-1]\n",
    "            ID_params = np.load(root+'\\\\'+f, allow_pickle=True).item()\n",
    "            cohort_params[ID] = ID_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c49a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_outcomes(deltaF):\n",
    "    final_outcomes = []\n",
    "    for index, rows in deltaF.iterrows():\n",
    "        rew = index[12]\n",
    "        outcome = index[16]\n",
    "        ttype = index[15]\n",
    "\n",
    "        if ttype == float(1):\n",
    "            if rew == float(1):\n",
    "                final = 'Tactile rewarded '+outcome\n",
    "            else:\n",
    "                final = 'Tactile unrewarded '+outcome\n",
    "        elif ttype == float(2):\n",
    "            if rew == float(1):\n",
    "                final = 'Auditory rewarded '+outcome\n",
    "            else:\n",
    "                final = 'Auditory unrewarded '+outcome        \n",
    "        else:\n",
    "            if rew == float(1):\n",
    "                final = 'Catch rewarded '+outcome\n",
    "            else:\n",
    "                final = 'Catch unrewarded '+outcome           \n",
    "        final_outcomes.append(final)\n",
    "    \n",
    "    deltaF.index = pd.MultiIndex.from_tuples([tuple(list(x) + [final_outcomes[i]]) for i, (x) in enumerate(deltaF.index)], names=deltaF.index.names+['Final Outcome'])           \n",
    "    \n",
    "    return deltaF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f09f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pupil = pd.concat(cohort_pupil)\n",
    "pupil_behavior = []\n",
    "for mouse in all_pupil.index.get_level_values(0).unique():\n",
    "    mouse_df = all_pupil[all_pupil.index.get_level_values(0) == mouse]\n",
    "    for session in mouse_df.index.get_level_values('Unique session').unique():\n",
    "        session_df = mouse_df[mouse_df.index.get_level_values('Unique session') == session]\n",
    "        protocol = session_df.index.get_level_values('Protocol')[0]\n",
    "        first_channel = session_df.index.get_level_values('Channel').unique()[0]\n",
    "        first_channel_df = session_df[session_df.index.get_level_values('Channel') == first_channel]\n",
    "        first_channel_df = first_channel_df.reset_index().iloc[:,:18]\n",
    "        first_channel_df = first_channel_df.rename({'level_0':'ID'}, axis=1)\n",
    "        first_channel_df['Anticipatory licks'] = [x+1 for x in list(first_channel_df['Anticipatory licks'])]\n",
    "        first_channel_df['Response latency'] = [(float(x)/1000)-3 for x in list(first_channel_df['Response latency'].replace({'-':np.nan}))]\n",
    "        posteriors_df = pd.DataFrame(calculate_posteriors(first_channel_df, first_channel_df.shape[0], protocol, 'pupil')[0])\n",
    "        hit_rate = posteriors_df['response rate']['G']\n",
    "        FA_rate = posteriors_df['response rate']['N']\n",
    "        correct = first_channel_df[(first_channel_df.Outcome  == 'hit') | (first_channel_df.Outcome  == 'CR')].shape[0]\n",
    "        CP_rate = correct/first_channel_df.shape[0]\n",
    "        FA_latency = (first_channel_df[first_channel_df.Outcome == 'FA']['Response latency']).mean()\n",
    "        hit_latency = (first_channel_df[first_channel_df.Outcome == 'hit']['Response latency']).mean()\n",
    "#         FA_licks = (first_ROI_df[first_ROI_df.Outcome == 'FA']['Anticipatory licks']).mean()\n",
    "#         hit_licks = (first_ROI_df[first_ROI_df.Outcome == 'hit']['Anticipatory licks']).mean()\n",
    "#         norm_licks = FA_licks/hit_licks\n",
    "#         FA_licks_rew = (first_ROI_df[(first_ROI_df.Outcome == 'FA') & (first_ROI_df.Water == float(1))]['Reward licks']).mean()\n",
    "#         hit_licks_rew = (first_ROI_df[(first_ROI_df.Outcome == 'hit') & (first_ROI_df.Water == float(1))]['Reward licks']).mean()\n",
    "#         final_df = pd.DataFrame([CP_rate, hit_rate, FA_rate, prop_rare, norm_licks, FA_licks, hit_licks, FA_licks_rew, hit_licks_rew, hit_latency, FA_latency, protocol], \n",
    "#                                 index=['CP rate','Hit rate','FA rate','Rare outcome proportion','Hit/FA licks', 'Hit ant', 'FA ant', 'Hit rew','FA rew', 'Hit latency','FA latency', 'Protocol'], \n",
    "#                                 columns=[mouse+'_'+session]).T\n",
    "#         final_df = final_df.reset_index().set_index(['index','Protocol'])\n",
    "#         imaging_behavior.append(final_df)\n",
    "# imaging_behavior = pd.concat(imaging_behavior)\n",
    "# imaging_behavior.index = imaging_behavior.index.set_names('FOV',level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33095fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deltaF = pd.concat(cohort_deltaF)\n",
    "deltaF = redefine_outcomes(all_deltaF)\n",
    "expectation = []\n",
    "for index, rows in deltaF.iterrows():\n",
    "    t = index[20]\n",
    "    if t in ['Tactile rewarded hit', 'Auditory rewarded hit', 'Tactile unrewarded FA', 'Auditory unrewarded FA']:\n",
    "        expectation.append('expected')\n",
    "    elif t in ['Tactile rewarded FA', 'Auditory rewarded FA', 'Tactile unrewarded hit', 'Auditory unrewarded hit']:\n",
    "        expectation.append('unexpected')\n",
    "    else:\n",
    "        expectation.append('no expectation')\n",
    "deltaF['expectation'] = expectation\n",
    "values = [x+(y,) for x,y in zip(deltaF.index.values, deltaF['expectation'])]\n",
    "names = list(deltaF.index.names)\n",
    "names.append('expectation')\n",
    "deltaF.index = pd.MultiIndex.from_tuples(values, names=names)\n",
    "deltaF = deltaF.iloc[:,:-1]\n",
    "deltaF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "extinction_df = deltaF[deltaF.index.get_level_values('Protocol') == 'Edet']\n",
    "for channel in ['Red']:\n",
    "    channel_df = extinction_df[extinction_df.index.get_level_values('Channel') == channel]\n",
    "    ROI_groups = channel_df.groupby(channel_df.index.get_level_values('Unique_ROI'))\n",
    "    for roi in channel_df.index.get_level_values('Unique_ROI').unique():\n",
    "        if roi == 'Edet_230101_FOV1.EDET_Red_215':\n",
    "            ROI_df= ROI_groups.get_group(roi)\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,4), sharey=True)      \n",
    "            for col, (ttype, color) in enumerate(zip(['rewarded hit','rewarded FA','unrewarded hit'],['navy','cyan','magenta'])):\n",
    "                ttype_df = ROI_df[(ROI_df.index.get_level_values('Final Outcome') == 'Tactile '+ttype) | (ROI_df.index.get_level_values('Final Outcome') == 'Auditory '+ttype)]\n",
    "                inferred = []\n",
    "                for index, rows in ttype_df.iterrows():\n",
    "                    fps = 30.3 if len(rows.dropna()) == 364 else 30.54\n",
    "                    if index[8] == '-':\n",
    "                        water = 4.5\n",
    "                    else:\n",
    "                        if index[13] == '-':\n",
    "                            water = int(index[8])/1000 + 1\n",
    "                        else:\n",
    "                            water = int(index[13])/1000\n",
    "                    inferred.append(rows.iloc[math.floor(water*fps)-30:math.floor(water*fps)+90].to_frame().reset_index().iloc[:,1:].T)\n",
    "                if len(inferred) > 0 :\n",
    "                    inferred_df = pd.concat(inferred)\n",
    "                    ax[col].plot(inferred_df.mean(), color=color, linewidth=3)\n",
    "                    for trial, rows in inferred_df.iterrows():\n",
    "                        if rows.mean() != float(-1):\n",
    "                            ax[col].plot(rows, color='black', linewidth=1, alpha=0.5)\n",
    "                    ax[col].set_title(ttype)\n",
    "                    ax[col].set_xticks(np.linspace(0,129,5))\n",
    "                    ax[col].set_xticklabels([str(int(x)) for x in np.linspace(-1,3,5)])\n",
    "\n",
    "            plt.suptitle(roi)\n",
    "#             fig.savefig(roi+'_EXAMPLE_traces.svg')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging_behavior = []\n",
    "for mouse in deltaF.index.get_level_values(0).unique():\n",
    "    mouse_df = deltaF[deltaF.index.get_level_values(0) == mouse]\n",
    "    for session in mouse_df.index.get_level_values('Unique session').unique():\n",
    "        session_df = mouse_df[mouse_df.index.get_level_values('Unique session') == session]\n",
    "        protocol = session_df.index.get_level_values('Protocol')[0]\n",
    "        first_ROI = session_df.index.get_level_values('ROI').unique()[0]\n",
    "        first_ROI_df = session_df[session_df.index.get_level_values('ROI') == first_ROI]\n",
    "        first_ROI_df = first_ROI_df.reset_index().iloc[:,:22]\n",
    "        first_ROI_df = first_ROI_df.rename({'level_0':'ID'}, axis=1)\n",
    "        first_ROI_df['Anticipatory licks'] = [x+1 for x in list(first_ROI_df['Anticipatory licks'])]\n",
    "        first_ROI_df['Response latency'] = [(float(x)/1000)-3 for x in list(first_ROI_df['Response latency'].replace({'-':np.nan}))]\n",
    "\n",
    "        posteriors_df = pd.DataFrame(calculate_posteriors(first_ROI_df, first_ROI_df.shape[0], protocol)[0])\n",
    "        prop_rare = posteriors_df['reward prob']['N'] / posteriors_df['reward prob']['G']\n",
    "        hit_rate = posteriors_df['response rate']['G']\n",
    "        FA_rate = posteriors_df['response rate']['N']\n",
    "        correct = first_ROI_df[(first_ROI_df.Outcome  == 'hit') | (first_ROI_df.Outcome  == 'CR')].shape[0]\n",
    "        CP_rate = correct/first_ROI_df.shape[0]\n",
    "        FA_latency = (first_ROI_df[first_ROI_df.Outcome == 'FA']['Response latency']).mean()\n",
    "        hit_latency = (first_ROI_df[first_ROI_df.Outcome == 'hit']['Response latency']).mean()\n",
    "        FA_licks = (first_ROI_df[first_ROI_df.Outcome == 'FA']['Anticipatory licks']).mean()\n",
    "        hit_licks = (first_ROI_df[first_ROI_df.Outcome == 'hit']['Anticipatory licks']).mean()\n",
    "        norm_licks = FA_licks/hit_licks\n",
    "        FA_licks_rew = (first_ROI_df[(first_ROI_df.Outcome == 'FA') & (first_ROI_df.Water == float(1))]['Reward licks']).mean()\n",
    "        hit_licks_rew = (first_ROI_df[(first_ROI_df.Outcome == 'hit') & (first_ROI_df.Water == float(1))]['Reward licks']).mean()\n",
    "        final_df = pd.DataFrame([CP_rate, hit_rate, FA_rate, prop_rare, norm_licks, FA_licks, hit_licks, FA_licks_rew, hit_licks_rew, hit_latency, FA_latency, protocol], \n",
    "                                index=['CP rate','Hit rate','FA rate','Rare outcome proportion','Hit/FA licks', 'Hit ant', 'FA ant', 'Hit rew','FA rew', 'Hit latency','FA latency', 'Protocol'], \n",
    "                                columns=[mouse+'_'+session]).T\n",
    "        final_df = final_df.reset_index().set_index(['index','Protocol'])\n",
    "        imaging_behavior.append(final_df)\n",
    "imaging_behavior = pd.concat(imaging_behavior)\n",
    "imaging_behavior.index = imaging_behavior.index.set_names('FOV',level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(imaging_behavior)\n",
    "#imaging_behavior.to_csv('imaging_behavior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rthresh = 0\n",
    "event_thresh = 0.25\n",
    "plot_and_extract_correlated_ROIs(cohort_correlations, cohort_events, cohort_deltaF, rthresh, event_thresh)\n",
    "correlated_ROIs = np.load('correlated_ROIs_'+str(rthresh)+'rthresh_'+str(event_thresh)+'eventthresh.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "all_events = pd.concat(cohort_events)\n",
    "all_events = all_events.reset_index()\n",
    "all_events.index = all_events['level_0']+'_'+all_events['Unique_ROI']\n",
    "unrewarded_FA = all_events[(all_events.Water == float(0)) & (all_events.Outcome == 'FA')]\n",
    "rewarded_FA = all_events[(all_events.Water == float(1)) & (all_events.Outcome == 'FA')]\n",
    "unrewarded_hit = all_events[(all_events.Water == float(0)) & (all_events.Outcome == 'hit')]\n",
    "rewarded_hit = all_events[(all_events.Water == float(1)) & (all_events.Outcome == 'hit')]\n",
    "rewarded = all_events[(all_events.Water == float(1))]\n",
    "unrewarded = all_events[(all_events.Water == float(0))]\n",
    "expected = pd.concat([rewarded_hit, unrewarded_FA])\n",
    "unexpected = pd.concat([rewarded_FA, unrewarded_hit])\n",
    "\n",
    "all_event_rates = []\n",
    "all_single_trials = {}\n",
    "for df, t in zip([all_events, unrewarded_FA, rewarded_FA, unrewarded_hit, rewarded_hit, expected, unexpected],['all','unrewarded FA', 'rewarded FA', 'unrewarded hit', 'rewarded hit', 'expected', 'unexpected']):\n",
    "    if t == 'all':\n",
    "        df = df[df.Stimulus != float(3)]\n",
    "\n",
    "        inferred = []\n",
    "        for index, rows in df.iterrows():\n",
    "            if rows['Response latency'] == '-':\n",
    "                water = 4.5\n",
    "            else:\n",
    "                if rows['Water_time'] == '-':\n",
    "                    water = int(rows['Response latency'])/1000 + 1\n",
    "                else:\n",
    "                    water = int(rows['Water_time'])/1000\n",
    "            inferred.append(water)\n",
    "        df['inferred'] = inferred\n",
    "    #     display(df.groupby('Unique_ROI').mean())\n",
    "    #     plt.figure()\n",
    "    #     plt.hist(df['inferred'])\n",
    "\n",
    "        df = df[(df['ev_onset'] > df['inferred']) & (df['ev_onset']-1 < df['inferred'])]\n",
    "\n",
    "        df['Response latency'] = [float(x)/1000 for x in list(df['Response latency'].replace({'-':np.nan}))]\n",
    "        single_trials = df.groupby([df['level_0'], df['Unique session'], df.Trial, df.Channel, df.Protocol, df.Outcome]).mean()\n",
    "        groups = df.groupby([df['level_0'], df['Unique session'], df.Trial, df.Channel, df.Protocol, df.Outcome])\n",
    "        nevents = list(groups.size().values)\n",
    "        single_trials['nEvents'] = nevents\n",
    "\n",
    "        nROIs = []\n",
    "        for index, rows in single_trials.iterrows():\n",
    "            n = len(deltaF[(deltaF.index.get_level_values(0) == index[0]) & (deltaF.index.get_level_values('Unique session') == index[1])].index.get_level_values('ROI').unique())\n",
    "            nROIs.append(n)\n",
    "        single_trials['nROIs'] = nROIs\n",
    "        single_trials['nEvents/nROIs'] = single_trials['nEvents']/single_trials['nROIs']\n",
    "        single_trials = single_trials.drop(['level_1', 'ROI', 'nEvents','nROIs', 'Water','Stimulus'], axis=1)\n",
    "        for chan in ['Red','Green']:\n",
    "            channel_single_trials = single_trials[single_trials.index.get_level_values('Channel') == chan]\n",
    "            if chan not in all_single_trials.keys():\n",
    "                all_single_trials[chan] = {}\n",
    "                all_single_trials[chan][t] = channel_single_trials\n",
    "            else:\n",
    "                all_single_trials[chan][t] = channel_single_trials\n",
    "#     df['ev_latency'] = df['ev_onset']-df['inferred']\n",
    "    \n",
    "#     if t in ['rewarded','unrewarded']:\n",
    "#         for channel in ['Red','Green']:\n",
    "#             channel_df = df[df['Channel'] == channel]\n",
    "#             plt.figure(figsize=(3.5,2))\n",
    "#             for state, fill, alpha, lw, color in zip(['N','E'], [False,True], [1,0.5], [3,1],['black','grey']):\n",
    "#                 state_df = channel_df[(channel_df['Protocol'].str.contains(state+'dis')) | (channel_df['Protocol'].str.contains(state+'rev'))]\n",
    "#                 sns.histplot(state_df['ev_latency'], bins=8, kde=True, \n",
    "#                              color=color, alpha=alpha, line_kws={'linewidth':lw}, stat='density', fill=fill, linewidth=lw, element='step')\n",
    "#             plt.ylim(0.3,0.7)\n",
    "#                 plt.axvline(1.2)\n",
    "#                 plt.axvline(2.5)\n",
    "#             plt.suptitle(channel+' '+t)\n",
    "#             fig.savefig(channel+'_'+t+'_event_histogram.svg')\n",
    "#     event_count = df.groupby(df.index.get_level_values(0)).size().to_frame()\n",
    "    \n",
    "#     if 'expected' in t:\n",
    "#         ttype_deltaF = deltaF[(deltaF.index.get_level_values('expectation') == t)]\n",
    "#     else:    \n",
    "#         ttype_deltaF = deltaF[(deltaF.index.get_level_values('Final Outcome') == 'Tactile '+t) | (deltaF.index.get_level_values('Final Outcome') == 'Auditory '+t)]\n",
    "        \n",
    "#     non_active_ROIs = []\n",
    "#     for mouse in ttype_deltaF.index.get_level_values(0).unique():\n",
    "#         mouse_df = ttype_deltaF[ttype_deltaF.index.get_level_values(0) == mouse]\n",
    "#         for roi in mouse_df.index.get_level_values('Unique_ROI').unique():\n",
    "#             if mouse+'_'+roi not in event_count.index.get_level_values(0):\n",
    "#                 non_active_ROIs.append(mouse+'_'+roi)\n",
    "#     non_active_df = pd.DataFrame([0]*len(non_active_ROIs), index=non_active_ROIs, columns=[0])\n",
    "#     all_ROIs_count = pd.concat([event_count,non_active_df])\n",
    "#     ROI_trial_count = ttype_deltaF.groupby([ttype_deltaF.index.get_level_values(0), ttype_deltaF.index.get_level_values('Unique_ROI'), ttype_deltaF.index.get_level_values('Protocol')]).size().to_frame()\n",
    "#     ROI_trial_count = ROI_trial_count.reset_index()\n",
    "#     ROI_trial_count['Final index'] = ROI_trial_count['level_0']+'_'+ROI_trial_count['Unique_ROI']\n",
    "#     ROI_trial_count = ROI_trial_count.set_index(['Final index','Protocol'])\n",
    "#     ROI_trial_count = ROI_trial_count.iloc[:,2].to_frame().sort_index(level=0)\n",
    "#     all_ROIs_count = all_ROIs_count.sort_index(level=0)\n",
    "#     all_ROIs_count.index = ROI_trial_count.index\n",
    "#     event_rate = all_ROIs_count/ROI_trial_count\n",
    "#     event_rate = event_rate.rename({0:t}, level=0, axis=1)\n",
    "    \n",
    "#     ttype_dfs = []\n",
    "#     for protocol in event_rate.index.get_level_values('Protocol').unique():\n",
    "#         protocol_rate = event_rate[event_rate.index.get_level_values('Protocol') == protocol]\n",
    "#         ttype_dfs.append(protocol_rate)\n",
    "#     ttype_df = pd.concat(ttype_dfs)\n",
    "#     all_event_rates.append(ttype_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf534027",
   "metadata": {},
   "source": [
    "## Make this previous trial, and add correlated/uncorrelated ROI label\n",
    "## Then try outcome encoding averaged over the last 10 trials for each ROI as an individual feature instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_channels = []\n",
    "for channel, channel_dict in all_single_trials.items():\n",
    "    channel_df = pd.concat(channel_dict)\n",
    "    channel_df = channel_df[channel_df.index.get_level_values(0) == 'all']\n",
    "    for index, rows in channe\n",
    "#     both_channels.append(channel_df)\n",
    "# both_df = pd.concat(both_channels)\n",
    "# both_df['ev_latency'] = both_df['ev_onset'] - both_df['inferred']\n",
    "# both_df.to_csv('reward_all_single_trials.csv')\n",
    "        \n",
    "#     if rows['Response latency'] isnan == True:\n",
    "#     print(type(rows['Response latency']))\n",
    "#     channel_df.to_csv(channel+'_delayed_reward_single_trial_correlations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV_norm_means = {}\n",
    "all_reward_events = pd.concat(all_event_rates, axis=1)\n",
    "all_reward_events\n",
    "all_reward_events = all_reward_events.replace({0:np.nan})\n",
    "for soma, soma_list in somas.items():\n",
    "    if len(soma_list) == 0:\n",
    "        all_reward_events = all_reward_events[~(all_reward_events.index.get_level_values(0).str.contains(soma))]\n",
    "    else:\n",
    "        FOV = soma_list[0]\n",
    "        all_reward_events = all_reward_events[~((all_reward_events.index.get_level_values(0).str.contains(soma)) & (all_reward_events.index.get_level_values(0).str.contains(FOV)))]\n",
    "all_reward_events = all_reward_events[~(all_reward_events.index.get_level_values(0).str.contains('RGECO_GCamP_Batch1_muji_Edis_221104_FOV3.1'))]\n",
    "\n",
    "for channel in ['Red','Green']:\n",
    "    channel_df = all_reward_events[all_reward_events.index.get_level_values(0).str.contains(channel)]\n",
    "    correlated_dfs = []\n",
    "    uncorrelated_dfs =[]\n",
    "    for index, rows in channel_df.iterrows():\n",
    "        mouse = ('_').join(index[0].split('_')[:4])\n",
    "        ROI = index[0].split(mouse)[1][1:]\n",
    "\n",
    "        if len(correlated_ROIs[mouse].keys()) > 0:\n",
    "            if ROI in correlated_ROIs[mouse].keys():\n",
    "                correlated_dfs.append(rows.to_frame().T)\n",
    "            else:\n",
    "                uncorrelated_dfs.append(rows.to_frame().T)\n",
    "        else:\n",
    "                uncorrelated_dfs.append(rows.to_frame().T)            \n",
    "    correlated_df = pd.concat(correlated_dfs)\n",
    "    correlated_df.index.names = channel_df.index.names\n",
    "    uncorrelated_df = pd.concat(uncorrelated_dfs)\n",
    "    uncorrelated_df.index.names = channel_df.index.names\n",
    "    \n",
    "    for label, df in zip(['correlated_ROIs','uncorrelated_ROIs','ALL_ROIs'], [correlated_df, uncorrelated_df, channel_df]):\n",
    "        df['FOV'] = [('_').join(x.split('_')[:-2]) for x in list(df.index.get_level_values('Final index'))]\n",
    "        df = df.reset_index().set_index(['FOV','Final index','Protocol'])\n",
    "        FOV_means = df.groupby([df.index.get_level_values('FOV'), df.index.get_level_values('Protocol')]).mean()\n",
    "        norm_type = {}\n",
    "        for ttype in FOV_means.columns:\n",
    "            norm_type[ttype] = FOV_means['rewarded hit']\n",
    "        normalizer = pd.concat(norm_type, axis=1)\n",
    "        norm_df = df.div(normalizer)\n",
    "        if label == 'ALL_ROIs':\n",
    "#             protocol_df = norm_df[(norm_df.index.get_level_values('Protocol').str.contains('dis')) | (norm_df.index.get_level_values('Protocol').str.contains('rev'))]\n",
    "            protocol_df = norm_df\n",
    "            FOV_norms = protocol_df.groupby([protocol_df.index.get_level_values('FOV'), protocol_df.index.get_level_values('Protocol')]).mean()\n",
    "            correlation_df = pd.merge(FOV_norms, imaging_behavior, left_index=True, right_index=True)\n",
    "            correlation_df = correlation_df[['rewarded FA', 'unrewarded hit', 'rewarded hit','CP rate','Hit rate','FA rate','Rare outcome proportion','Hit/FA licks']]\n",
    "            correlation_df.to_csv(channel+'_behavior_population_reward_calcium_correlation.csv')\n",
    "            df.to_csv(('_').join([label,channel,'reward_event_rate.csv']))\n",
    "            norm_df.to_csv(('_').join([label,channel,'reward_NORM_event_rate.csv']))\n",
    "        else:\n",
    "            df.to_csv(('_').join([label,str(event_thresh),channel,'reward_event_rate.csv']))\n",
    "            norm_df.to_csv(('_').join([label,str(event_thresh),channel,'reward_NORM_event_rate.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'deltaF_pupil.npy' in os.listdir(cwd):\n",
    "    print('deltaF_pupil.npy file found')\n",
    "    print('deltaF_pupil.npy file uploading now...')\n",
    "    deltaF_pupil = np.load('deltaF_pupil.npy', allow_pickle=True)\n",
    "    print('finished uploading')\n",
    "\n",
    "else:\n",
    "    print('deltaF_pupil.npy file not found')\n",
    "    print('deltaF_pupil.npy file creating now...')    \n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    deltaF_df = pd.concat(cohort_deltaF)\n",
    "    pupil_df = pd.concat(cohort_pupil)\n",
    "    deltaF_pupil = {}\n",
    "    mouse_groups = deltaF_df.groupby(deltaF_df.index.get_level_values(0))\n",
    "    for mouse in deltaF_df.index.get_level_values(0).unique():\n",
    "        deltaF_pupil[mouse] = {}\n",
    "        mouse_df = mouse_groups.get_group(mouse)\n",
    "        ROI_groups = mouse_df.groupby(mouse_df.index.get_level_values('Unique_ROI'))\n",
    "        mouse_correlation = {}\n",
    "\n",
    "        for i, (ROI) in enumerate(mouse_df.index.get_level_values('Unique_ROI').unique()):\n",
    "            ROI_df = ROI_groups.get_group(ROI)\n",
    "            deltaF_list = []\n",
    "            pupil_list = []\n",
    "            df_dict = {}\n",
    "            for j, (index, rows) in enumerate(ROI_df.iterrows()):\n",
    "                midx_number = np.where(np.asarray(ROI_df.index.names) == 'Unique_trial')[0][0]\n",
    "                unique_trial = index[midx_number]\n",
    "                mouse_pupil = pupil_df[pupil_df.index.get_level_values(0) == mouse]\n",
    "\n",
    "                if unique_trial in mouse_pupil.index.get_level_values('Unique_trial'):\n",
    "                    trial_pupil = np.asarray(mouse_pupil[mouse_pupil.index.get_level_values('Unique_trial') == unique_trial].iloc[0])\n",
    "                    downsampled_pupil = np.asarray([np.mean(x) for x in list(split(trial_pupil, rows.shape[0]))])\n",
    "                    trial_df = pd.DataFrame([np.asarray(rows), downsampled_pupil], index=['deltaF', 'pupil']).T\n",
    "                    df_dict[unique_trial] = trial_df\n",
    "            mouse_correlation = pd.concat(df_dict) if len(df_dict.keys()) > 0 else pd.DataFrame([], index=['deltaF', 'pupil']).T\n",
    "            deltaF_pupil[mouse][ROI] = mouse_correlation\n",
    "    pickle.dump(deltaF_pupil, open(\"deltaF_pupil.npy\", \"wb\"))\n",
    "\n",
    "    end = time.time()\n",
    "    print(end-start, 'seconds taken to create')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1861b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, epoch_dict in cohort_epochs['pupil'].items():\n",
    "    if epoch == 'all':\n",
    "        epoch_df = pd.concat(epoch_dict).dropna(how='all')\n",
    "        dfs = []\n",
    "        for index, rows in epoch_df.iterrows():\n",
    "            water = index[12]\n",
    "            lick = index[7]\n",
    "            \n",
    "            if lick == '-':\n",
    "                water = 4500\n",
    "            else:\n",
    "                if water == '-':\n",
    "                    water = int(lick)+1000\n",
    "                else:\n",
    "                    water = int(water)\n",
    "            water_fps = math.floor(water/1000*30*30)\n",
    "            dfs.append(rows.iloc[water_fps:water_fps+1800].to_numpy())\n",
    "        df = pd.DataFrame(dfs, index=epoch_df.index, columns=list(range(900)))\n",
    "        \n",
    "        peak_df = df.max(axis=1).to_frame()\n",
    "        peak_df['Response latency'] = peak_df.index.get_level_values('Response latency')\n",
    "        peak_df['Prestim licks'] = peak_df.index.get_level_values('Prestim licks')\n",
    "        peak_df['Anticipatory licks'] = peak_df.index.get_level_values('Anticipatory licks')\n",
    "        peak_df['Reward licks'] = peak_df.index.get_level_values(' Reward licks')\n",
    "        peak_df = peak_df.droplevel(['Response latency','Prestim licks','Anticipatory licks',' Reward licks'])\n",
    "        peak_df = peak_df.rename({0:'Peak dilation'}, axis=1)\n",
    "\n",
    "        ttype_peaks = []\n",
    "        for t in ['rewarded hit', 'unrewarded hit', 'rewarded FA', 'unrewarded FA']:\n",
    "            if t in ['rewarded hit','rewarded FA']:\n",
    "                ttype_peak = peak_df[(peak_df.index.get_level_values('Final Outcome') == 'Tactile '+t) | (peak_df.index.get_level_values('Final Outcome') == 'Auditory '+t)].T\n",
    "            else:    \n",
    "                ttype_peak = peak_df[peak_df.index.get_level_values('Final Outcome').str.contains(t)].T\n",
    "            ttype_peak.index = pd.MultiIndex.from_tuples((x,y) for x,y in zip(ttype_peak.index, [t]*ttype_peak.shape[0]))\n",
    "            ttype_peak = ttype_peak.T.droplevel('Final Outcome')\n",
    "            ttype_peaks.append(ttype_peak)\n",
    "        ttype_peak_df = pd.concat(ttype_peaks, axis=1).sort_index(level=0, axis=1)\n",
    "    ttype_peak_df.to_csv('delayed_reward_SINGLE_TRIAL_PUPIL.csv')\n",
    "\n",
    "    sesh_by_sesh = ttype_peak_df.groupby([ttype_peak_df.index.get_level_values(0), ttype_peak_df.index.get_level_values('Unique session'), ttype_peak_df.index.get_level_values('Protocol')]).mean()\n",
    "    sesh_by_sesh['sesh'] = sesh_by_sesh.index.get_level_values(0) + '_' + sesh_by_sesh.index.get_level_values('Unique session')\n",
    "    sesh_by_sesh.to_csv('delayed_reward_sesh_by_sesh_PUPIL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, epoch_dict in cohort_epochs['events'].items():\n",
    "    if epoch == 'all':\n",
    "        epoch_df = pd.concat(epoch_dict)\n",
    "        \n",
    "        for soma, soma_list in somas.items():\n",
    "            if len(soma_list) == 0:\n",
    "                epoch_df = epoch_df[epoch_df.index.get_level_values(0) != soma]\n",
    "            else:\n",
    "                FOV = soma_list[0]\n",
    "                epoch_df = epoch_df[~((epoch_df.index.get_level_values(0).str.contains(soma)) & (epoch_df['FOV'].str.contains(FOV)))]\n",
    "        \n",
    "\n",
    "        deltaF_df = pd.concat(cohort_epochs['deltaF'][epoch])\n",
    "        deltaF_dfs = []\n",
    "        for index, rows in deltaF_df.iterrows():\n",
    "            water = index[12]\n",
    "            lick = index[7]\n",
    "\n",
    "            if lick == '-':\n",
    "                water = 4500\n",
    "            else:\n",
    "                if water == '-':\n",
    "                    water = int(lick)+1000\n",
    "                else:\n",
    "                    water = int(water)\n",
    "            water_fps = math.floor(water/1000*30.3)\n",
    "            deltaF_dfs.append(rows.iloc[water_fps:water_fps+30].to_numpy())\n",
    "        deltaF_df = pd.DataFrame(deltaF_dfs, index=deltaF_df.index, columns=list(range(30)))\n",
    "        \n",
    "        for protocol in ['Ndet','Ndis', 'Nrev']:\n",
    "            plt.figure()\n",
    "            legends = []\n",
    "            for t, c in zip(['rewarded hit', 'unrewarded hit', 'rewarded FA', 'unrewarded FA'], ['navy','magenta','cyan','orange']):\n",
    "                ttype_df = deltaF_df[(deltaF_df.index.get_level_values('Final Outcome') == 'Tactile '+t) | (deltaF_df.index.get_level_values('Final Outcome') == 'Auditory '+t)]\n",
    "                ttype_df = ttype_df[ttype_df.index.get_level_values('Protocol') == protocol]\n",
    "                ttype_df = ttype_df[ttype_df.index.get_level_values('Channel') == 'Red']\n",
    "                plt.plot(list(range(30)), ttype_df.mean(), color=c)\n",
    "                plt.fill_between(list(range(30)), ttype_df.mean()-ttype_df.sem(), ttype_df.mean()+ttype_df.sem(), color=c, alpha=0.5)\n",
    "                legends.append(t)\n",
    "                legends.append(t)\n",
    "            plt.legend(legends)\n",
    "            plt.suptitle(protocol)\n",
    "\n",
    "\n",
    "#         correlated_dfs = []\n",
    "#         for index, rows in epoch_df.iterrows():\n",
    "#             mouse = index[0]\n",
    "#             ROI = rows['Unique_ROI']\n",
    "\n",
    "#             if len(correlated_ROIs[mouse].keys()) > 0:\n",
    "#                 if ROI in correlated_ROIs[mouse].keys():\n",
    "#                     correlated_dfs.append(rows.to_frame().T)\n",
    "\n",
    "#         if len(correlated_dfs) == 0:\n",
    "#             correlated_df = epoch_df\n",
    "#         else:\n",
    "#             correlated_df = pd.concat(correlated_dfs)\n",
    "#             correlated_df.index.names = correlated_df.index.names\n",
    "\n",
    "#         for label, df in zip(['ALL_ROIs','correlated_ROIs'],[epoch_df, correlated_df]):\n",
    "#             if label == 'ALL_ROIs':\n",
    "#                 epoch_event_rates = []\n",
    "#                 epoch_event_metrics_list = []\n",
    "#                 plt.figure()\n",
    "#                 legends = []\n",
    "#                 for t in ['rewarded hit', 'unrewarded hit', 'rewarded FA', 'unrewarded FA']:\n",
    "#                     ttype_events = df[(df['Final Outcome'] == 'Tactile '+t) | (df['Final Outcome'] == 'Auditory '+t)]\n",
    "#                     ROI_event_count = ttype_events.groupby([ttype_events.index.get_level_values(0), 'Unique_ROI', 'Protocol']).size()\n",
    "#                     plt.hist(ROI_event_count, alpha=0.5)\n",
    "#                     legends.append(t)\n",
    "#                 plt.legend(legends)\n",
    "#                 ROI_deltaF_df = deltaF_df[deltaF_df.index.get_level_values('Unique_ROI').isin(list(ROI_event_count.index.get_level_values('Unique_ROI')))]\n",
    "#                 ttype_deltaF = ROI_deltaF_df[ROI_deltaF_df.index.get_level_values('Final Outcome').str.contains(t)]\n",
    "#                 ROI_trial_count = ttype_deltaF.groupby([ttype_deltaF.index.get_level_values(0), ttype_deltaF.index.get_level_values('Unique_ROI'), ttype_deltaF.index.get_level_values('Protocol')]).size()\n",
    "#                 ROI_event_rate = ROI_event_count / ROI_trial_count\n",
    "#                 ROI_event_rate_df = pd.DataFrame(ROI_event_rate.values, index=ROI_event_rate.index, columns=[t])\n",
    "#                 epoch_event_rates.append(ROI_event_rate_df)\n",
    "#                 epoch_event_means = ttype_events.groupby([ttype_events.index.get_level_values(0), 'Unique_ROI', 'Final Outcome','Protocol']).mean()\n",
    "\n",
    "#                 outcome_df = ttype_events.groupby([ttype_events.index.get_level_values(0),ttype_events['Unique_ROI'], ttype_events['Protocol']]).mean().T.reset_index()\n",
    "#                 outcome_df.index = pd.MultiIndex.from_tuples([(x,y) for x,y in zip(outcome_df['index'], [t]*outcome_df.shape[0])])\n",
    "#                 outcome_df = outcome_df.T\n",
    "#                 epoch_event_metrics_list.append(outcome_df)\n",
    "#             epoch_event_metrics = pd.concat(epoch_event_metrics_list, axis=1).T\n",
    "\n",
    "#             epoch_event_metric_list = []\n",
    "#             for metric in ['response latency', 'Prestim licks', 'Anticipatory licks', 'Reward licks', 'Stimulus', 'peak', 'ev_onset', 'peak_time', 'integral']:\n",
    "#                 metric_df = epoch_event_metrics[epoch_event_metrics.index.get_level_values(0) == metric]\n",
    "#                 epoch_event_metric_list.append(metric_df)\n",
    "#             epoch_event_metrics = pd.concat(epoch_event_metric_list).T\n",
    "#             epoch_event_rate = pd.concat(epoch_event_rates, axis=1)\n",
    "\n",
    "#             for channel in ['Red','Green']:\n",
    "#                 channel_epoch_event_rate = epoch_event_rate[epoch_event_rate.index.get_level_values('Unique_ROI').str.contains(channel)]\n",
    "#                 channel_epoch_event_metrics = epoch_event_metrics[epoch_event_metrics.index.get_level_values('Unique_ROI').str.contains(channel)]\n",
    "\n",
    "#                 channel_epoch_event_rate.to_csv(('_').join([label,channel,epoch,'event_rate.csv']))\n",
    "#                 channel_epoch_event_metrics.to_csv(('_').join([label,channel,epoch,'event_metrics.csv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d988b",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pupil(data, epochs, protocols, outcomes, plottype, trials, event_epochs=None):\n",
    "    data = data['pupil']\n",
    "    if type(epochs) == str:\n",
    "        if epochs == 'all':\n",
    "            epochs = data.keys()\n",
    "    else:   \n",
    "        epochs = epochs\n",
    "        \n",
    "    for epoch, epoch_dict in data.items():\n",
    "        if epoch in epochs:\n",
    "            epoch_df = pd.concat(epoch_dict)\n",
    "            epoch_df = epoch_df[epoch_df.index.get_level_values('Stimulus') != float(3)]   \n",
    "            \n",
    "            if type(protocols) == str:\n",
    "                if protocols == 'all':\n",
    "                    protocols = ['N','E'] + list(epoch_df.index.get_level_values('Protocol').unique())\n",
    "            else:\n",
    "                protocols = protocols\n",
    "            \n",
    "            for protocol in protocols:\n",
    "                view_outcomes = []\n",
    "                labels = []\n",
    "                \n",
    "                for outcome in outcomes:\n",
    "                    if (outcomes == ['FA', 'hit']) | (outcomes == ['hit', 'FA']):\n",
    "                        for label, reward in zip(['rewarded', 'unrewarded'],[float(1), float(0)]):\n",
    "                            rew_df = epoch_df[epoch_df.index.get_level_values('Water') == reward]\n",
    "                            out_df = rew_df[rew_df.index.get_level_values('Final Outcome').str.contains(outcome)]\n",
    "                            view_outcomes.append(out_df)\n",
    "                            labels.append(label+' '+outcome)\n",
    "                    else:\n",
    "                            out_df = epoch_df[epoch_df.index.get_level_values('Final Outcome').str.contains(outcome)]\n",
    "                            view_outcomes.append(out_df)\n",
    "                            labels.append(outcome)                            \n",
    "                \n",
    "                n = len(view_outcomes)\n",
    "                fig, ax = plt.subplots(nrows=1, ncols=n, figsize=(5*n,3), sharey=True)\n",
    "                if plottype == 'trials':\n",
    "                    color = cm.rainbow(np.linspace(0, 1, n)) \n",
    "                    for col, (outcome_df, color, label) in enumerate(zip(view_outcomes, color, labels)):\n",
    "                        outcome_df = outcome_df[outcome_df.index.get_level_values('Protocol').str.contains(protocol)]\n",
    "                        \n",
    "                        if trials == 'events':\n",
    "                            outcome_df = get_event_trials(outcome_df, 'pupil', event_epochs)\n",
    "                        else:\n",
    "                            outcome_df = outcome_df\n",
    "                        \n",
    "                        nmice = len(outcome_df.index.get_level_values(0).unique())\n",
    "                        nsessions = len(outcome_df.index.get_level_values('Unique session').unique())\n",
    "                        mouse_counts = {}\n",
    "                        for mouse in outcome_df.index.get_level_values(0).unique():\n",
    "                            mouse_counts[mouse] = len(outcome_df[outcome_df.index.get_level_values(0) == mouse].index.get_level_values('Unique_trial').unique())\n",
    "                            \n",
    "                        unique = outcome_df.groupby(outcome_df.index.get_level_values('Unique_trial')).mean()\n",
    "                        ntrials = unique.shape[0]\n",
    "                        mean = unique.mean()\n",
    "                        sem = unique.sem()\n",
    "                        ax[col].plot(mean, color=color)\n",
    "                        ax[col].fill_between(x=list(range(unique.shape[1])), y1=mean-sem, y2=mean+sem, color=color, alpha=0.5)\n",
    "                        if epoch == 'all':\n",
    "                            ax[col].set_xticks(np.linspace(0,unique.shape[1],13))\n",
    "                            ax[col].set_xticklabels([int(x) for x in np.linspace(0,12,13)])      \n",
    "                        else:\n",
    "                            ax[col].set_xticks(np.linspace(0,unique.shape[1],11))\n",
    "                            ax[col].set_xticklabels([str(round(x,1)) for x in np.linspace(0,1,11)])\n",
    "                        \n",
    "                        title = str(nsessions)+' sessions/'+str(ntrials)+' trials \\n '                     \n",
    "                        for i, (key, value) in enumerate(mouse_counts.items()):\n",
    "                            title += ' \\n '+key+': '+str(value)+' trials'\n",
    "                        ax[col].set_title(title)\n",
    "                        ax[col].legend([label])\n",
    "                        \n",
    "                else:\n",
    "                    legends = []\n",
    "                    for col, (outcome_df, label) in enumerate(zip(view_outcomes, labels)):\n",
    "                        outcome_df = outcome_df[outcome_df.index.get_level_values('Protocol').str.contains(protocol)]\n",
    "                        \n",
    "                        if trials == 'events':\n",
    "                            outcome_df = get_event_trials(outcome_df, 'pupil', event_epochs)\n",
    "                        else:\n",
    "                            outcome_df = outcome_df\n",
    "                            \n",
    "                        nmice = len(list(outcome_df.index.get_level_values(0).unique()))\n",
    "                        color = cm.rainbow(np.linspace(0,1,nmice))                        \n",
    "                        unique = outcome_df.groupby(outcome_df.index.get_level_values(0)).mean() \n",
    "                        \n",
    "                        for j, ((color), (index, rows)) in enumerate(zip(color, unique.iterrows())):\n",
    "                            ax[col].plot(rows, color=ID_colors[index])\n",
    "                            if epoch == 'all':\n",
    "                                ax[col].set_xticks(np.linspace(0,unique.shape[1],13))\n",
    "                                ax[col].set_xticklabels([int(x) for x in np.linspace(0,12,13)])      \n",
    "                            else:\n",
    "                                ax[col].set_xticks(np.linspace(0,unique.shape[1],11))\n",
    "                                ax[col].set_xticklabels([str(round(x,1)) for x in np.linspace(0,1,11)])                \n",
    "                            ax[col].set_title(label)\n",
    "                            legends.append(index)\n",
    "                        ax[col].legend(legends, prop={'size': 6})\n",
    "                if trials == 'all':\n",
    "                    plt.suptitle(protocol+' '+epoch+' epoch (Pupil) using all trials', fontsize=25, y=1.5)\n",
    "                else:\n",
    "                    plt.suptitle(protocol+' '+epoch+' epoch (Pupil) using only trials with '+event_epochs+' events', fontsize=25, y=1.5)\n",
    "                plt.ylim(0.94,1.2)\n",
    "                fig.savefig(protocol+'_pupil.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calcium(data, channel, epochs, protocols, outcomes, plottype, trials, event_epochs=None):\n",
    "    data = data['deltaF']\n",
    "\n",
    "    if type(epochs) == str:\n",
    "        if epochs == 'all':\n",
    "            data.keys()\n",
    "    else:   \n",
    "        epochs = epochs\n",
    "        \n",
    "    for epoch, epoch_dict in data.items():\n",
    "        if epoch in epochs:\n",
    "            epoch_df = pd.concat(epoch_dict)\n",
    "            epoch_df = epoch_df[epoch_df.index.get_level_values('Stimulus') != float(3)]  \n",
    "            \n",
    "            groups = epoch_df.groupby(epoch_df.index.get_level_values(0))\n",
    "            for group in epoch_df.index.get_level_values(0).unique():\n",
    "                group_df = groups.get_group(group)\n",
    "            epoch_df = epoch_df[epoch_df.index.get_level_values('Channel') == channel]\n",
    "            \n",
    "            if channel == 'Green':\n",
    "                epoch_df = epoch_df\n",
    "            else:\n",
    "                for soma, soma_list in somas.items():\n",
    "                    if len(soma_list) == 0:\n",
    "                        epoch_df = epoch_df[epoch_df.index.get_level_values(0) != soma]\n",
    "                    else:\n",
    "                        FOV = soma_list[0]\n",
    "                        epoch_df = epoch_df[~((epoch_df.index.get_level_values(0).str.contains(soma)) & (epoch_df.index.get_level_values('FOV').str.contains(FOV)))]\n",
    "            \n",
    "            if type(protocols) == str:\n",
    "                if protocols == 'all':\n",
    "                    protocols = ['N','E'] + list(epoch_df.index.get_level_values('Protocol').unique())\n",
    "            else:\n",
    "                protocols = protocols\n",
    "            \n",
    "            for protocol in protocols:\n",
    "                view_outcomes = []\n",
    "                labels = []\n",
    "                for outcome in outcomes:\n",
    "                    if (outcomes == ['FA', 'hit']) | (outcomes == ['hit', 'FA']):\n",
    "                        if epoch in ['reward', 'ITI', 'delayed_reward', 'all']:\n",
    "                            for label, reward in zip(['rewarded', 'unrewarded'],[float(1), float(0)]):\n",
    "                                rew_df = epoch_df[epoch_df.index.get_level_values('Water') == reward]\n",
    "                                out_df = rew_df[rew_df.index.get_level_values('Final Outcome').str.contains(outcome)]\n",
    "                                view_outcomes.append(out_df)\n",
    "                                labels.append(label+' '+outcome)\n",
    "                        else:\n",
    "                            out_df = epoch_df[epoch_df.index.get_level_values('Final Outcome').str.contains(outcome)]\n",
    "                            view_outcomes.append(out_df)\n",
    "                            labels.append(outcome)     \n",
    "                    elif outcome == 'rewarded':\n",
    "                        out_df = epoch_df[epoch_df.index.get_level_values('Water') == float(1)]\n",
    "                        view_outcomes.append(out_df)\n",
    "                        labels.append(outcome)   \n",
    "                    elif outcome == 'unrewarded':                        \n",
    "                        out_df = epoch_df[epoch_df.index.get_level_values('Water') == float(0)]\n",
    "                        view_outcomes.append(out_df)\n",
    "                        labels.append(outcome)                           \n",
    "                    else:\n",
    "                            out_df = epoch_df[epoch_df.index.get_level_values('Final Outcome').str.contains(outcome)]\n",
    "                            view_outcomes.append(out_df)\n",
    "                            labels.append(outcome)                   \n",
    "                n = len(view_outcomes)\n",
    "                fig, ax = plt.subplots(nrows=1, ncols=n, figsize=(5*n,3), sharey=True)\n",
    "                if plottype == 'ROIs':\n",
    "                    color = cm.rainbow(np.linspace(0, 1, n)) \n",
    "                    for col, (outcome_df, color, label) in enumerate(zip(view_outcomes, color, labels)):\n",
    "                        if protocol =='E2':\n",
    "                            outcome_df = outcome_df[(outcome_df.index.get_level_values('Protocol') == 'Edis') | (outcome_df.index.get_level_values('Protocol') == 'Erev')]\n",
    "                        elif protocol =='N2':\n",
    "                            outcome_df = outcome_df[(outcome_df.index.get_level_values('Protocol') == 'Ndis') | (outcome_df.index.get_level_values('Protocol') == 'Nrev')]\n",
    "                        elif protocol == 'dynamic':\n",
    "                            outcome_df = outcome_df          \n",
    "                        elif protocol == 'stable':\n",
    "                            outcome_df = outcome_df\n",
    "                        else:\n",
    "                            outcome_df = outcome_df[outcome_df.index.get_level_values('Protocol').str.contains(protocol)]\n",
    "                        \n",
    "#                         if protocol == 'dynamic':\n",
    "#                             outcome_dfs = []\n",
    "#                             sessions = outcome_df.groupby(outcome_df.index.get_level_values('Unique session'))\n",
    "#                             for sesh in outcome_df.index.get_level_values('Unique session').unique():\n",
    "#                                 shortened_sesh = sesh.split('FOV')[0][:-1]\n",
    "#                                 sesh_df = sessions.get_group(sesh)\n",
    "#                                 check_ID = sesh_df.index.get_level_values(0)[0]\n",
    "#                                 for dysesh in short_term_strategy:\n",
    "#                                     if (check_ID in dysesh) & (shortened_sesh in dysesh) & ('PUPIL' not in dysesh):\n",
    "#                                         outcome_dfs.append(sesh_df)\n",
    "#                             outcome_df = pd.concat(outcome_dfs)\n",
    "#                         elif protocol == 'stable':\n",
    "#                             dynamic_seshes = []\n",
    "#                             sessions = outcome_df.groupby(outcome_df.index.get_level_values('Unique session'))\n",
    "#                             for sesh in outcome_df.index.get_level_values('Unique session').unique():\n",
    "#                                 shortened_sesh = sesh.split('FOV')[0][:-1]\n",
    "#                                 sesh_df = sessions.get_group(sesh)\n",
    "#                                 check_ID = sesh_df.index.get_level_values(0)[0]\n",
    "#                                 for dysesh in short_term_strategy:\n",
    "#                                     if (check_ID in dysesh) & (shortened_sesh in dysesh) & ('PUPIL' not in dysesh):\n",
    "#                                         dynamic_seshes.append(sesh)\n",
    "\n",
    "#                             outcome_dfs = []\n",
    "#                             for sesh in outcome_df.index.get_level_values('Unique session').unique():\n",
    "#                                 sesh_df = sessions.get_group(sesh)\n",
    "#                                 if sesh in dynamic_seshes:\n",
    "#                                     None\n",
    "#                                 else:\n",
    "#                                     outcome_dfs.append(sesh_df)\n",
    "#                             outcome_df = pd.concat(outcome_dfs)\n",
    "#                         else:\n",
    "#                             outcome_df = outcome_df  \n",
    "                            \n",
    "                        if trials == 'events':\n",
    "                            outcome_df = get_event_trials(outcome_df, 'deltaF', event_epochs)\n",
    "                        else:\n",
    "                            outcome_df = outcome_df\n",
    "                        \n",
    "                        mouse_counts = {}\n",
    "                        \n",
    "                        for mouse in outcome_df.index.get_level_values(0).unique():\n",
    "                            mouse_counts[mouse] = len(outcome_df[outcome_df.index.get_level_values(0) == mouse].index.get_level_values('Unique_ROI').unique())\n",
    "                            \n",
    "                        nmice = len(outcome_df.index.get_level_values(0).unique())\n",
    "                        nsessions = len(outcome_df.index.get_level_values('Unique session').unique())\n",
    "                        \n",
    "                        unique = outcome_df.groupby(outcome_df.index.get_level_values('Unique_ROI')).mean()\n",
    "                        nROIs = unique.shape[0]\n",
    "                        mean = unique.mean()\n",
    "                        sem = unique.sem()\n",
    "                        ax[col].plot(mean, color=color)\n",
    "                        ax[col].fill_between(x=list(range(unique.shape[1])), y1=mean-sem, y2=mean+sem, color=color, alpha=0.5)\n",
    "                        \n",
    "                        if epoch == 'all':\n",
    "                            ax[col].set_xticks(np.linspace(0,unique.shape[1],13))\n",
    "                            ax[col].set_xticklabels([int(x) for x in np.linspace(0,12,13)])      \n",
    "                        else:\n",
    "                            ax[col].set_xticks(np.linspace(0,unique.shape[1],11))\n",
    "                            ax[col].set_xticklabels([str(round(x,1)) for x in np.linspace(0,1,11)])\n",
    "                        title = str(nsessions)+' sessions/'+str(nROIs)+' ROIs \\n '\n",
    "                        \n",
    "                        for i, (key, value) in enumerate(mouse_counts.items()):\n",
    "                            title += ' \\n '+key+': '+str(value)+' ROIs'\n",
    "                        ax[col].set_title(title)\n",
    "                        ax[col].legend([label])\n",
    "                elif plottype == 'mice':\n",
    "                    legends = []\n",
    "                    for col, (outcome_df, label) in enumerate(zip(view_outcomes, labels)):\n",
    "                        if protocol =='E2':\n",
    "                            outcome_df = outcome_df[(outcome_df.index.get_level_values('Protocol') == 'Edis') | (outcome_df.index.get_level_values('Protocol') == 'Erev')]\n",
    "                        elif protocol =='N2':\n",
    "                            outcome_df = outcome_df[(outcome_df.index.get_level_values('Protocol') == 'Ndis') | (outcome_df.index.get_level_values('Protocol') == 'Nrev')]\n",
    "                        else:\n",
    "                            outcome_df = outcome_df[outcome_df.index.get_level_values('Protocol').str.contains(protocol)]                        \n",
    "                        \n",
    "                        if trials == 'events':\n",
    "                            outcome_df = get_event_trials(outcome_df, 'deltaF', event_epochs)\n",
    "                        else:\n",
    "                            outcome_df = outcome_df\n",
    "                            \n",
    "                        nmice = len(list(outcome_df.index.get_level_values(0).unique()))\n",
    "                        color = cm.rainbow(np.linspace(0,1,nmice))                        \n",
    "                        unique = outcome_df.groupby(outcome_df.index.get_level_values(0)).mean() \n",
    "                        \n",
    "                        for j, ((color), (index, rows)) in enumerate(zip(color, unique.iterrows())):\n",
    "                            ax[col].plot(rows, color=ID_colors[index])\n",
    "                            if epoch == 'all':\n",
    "                                ax[col].set_xticks(np.linspace(0,unique.shape[1],13))\n",
    "                                ax[col].set_xticklabels([int(x) for x in np.linspace(0,12,13)])      \n",
    "                            else:\n",
    "                                ax[col].set_xticks(np.linspace(0,unique.shape[1],11))\n",
    "                                ax[col].set_xticklabels([str(round(x,1)) for x in np.linspace(0,1,11)])              \n",
    "                            ax[col].set_title(label)\n",
    "                            legends.append(index)\n",
    "                        ax[col].legend(legends, prop={'size': 6})\n",
    "                else:\n",
    "                    color = cm.rainbow(np.linspace(0, 1, n)) \n",
    "                    for col, (uncorrelated_df, color, label) in enumerate(zip(view_outcomes, color, labels)):\n",
    "                        if protocol =='E2':\n",
    "                            uncorrelated_df = uncorrelated_df[(uncorrelated_df.index.get_level_values('Protocol') == 'Edis') | (uncorrelated_df.index.get_level_values('Protocol') == 'Erev')]\n",
    "                        elif protocol =='N2':\n",
    "                            uncorrelated_df = uncorrelated_df[(uncorrelated_df.index.get_level_values('Protocol') == 'Ndis') | (uncorrelated_df.index.get_level_values('Protocol') == 'Nrev')]\n",
    "                        elif protocol == 'dynamic':\n",
    "                            uncorrelated_df = uncorrelated_df          \n",
    "                        elif protocol == 'stable':\n",
    "                            uncorrelated_df = uncorrelated_df\n",
    "                        else:\n",
    "                            uncorrelated_df = uncorrelated_df[uncorrelated_df.index.get_level_values('Protocol').str.contains(protocol)]\n",
    "                        \n",
    "#                         if protocol == 'dynamic':\n",
    "#                             uncorrelated_dfs = []\n",
    "#                             sessions = uncorrelated_df.groupby(uncorrelated_df.index.get_level_values('Unique session'))\n",
    "#                             for sesh in uncorrelated_df.index.get_level_values('Unique session').unique():\n",
    "#                                 shortened_sesh = sesh.split('FOV')[0][:-1]\n",
    "#                                 sesh_df = sessions.get_group(sesh)\n",
    "#                                 check_ID = sesh_df.index.get_level_values(0)[0]\n",
    "#                                 for dysesh in short_term_strategy:\n",
    "#                                     if (check_ID in dysesh) & (shortened_sesh in dysesh) & ('PUPIL' not in dysesh):\n",
    "#                                         uncorrelated_dfs.append(sesh_df)\n",
    "#                             uncorrelated_df = pd.concat(uncorrelated_dfs)\n",
    "#                         elif protocol == 'stable':\n",
    "#                             dynamic_seshes = []\n",
    "#                             sessions = uncorrelated_df.groupby(uncorrelated_df.index.get_level_values('Unique session'))\n",
    "#                             for sesh in uncorrelated_df.index.get_level_values('Unique session').unique():\n",
    "#                                 shortened_sesh = sesh.split('FOV')[0][:-1]\n",
    "#                                 sesh_df = sessions.get_group(sesh)\n",
    "#                                 check_ID = sesh_df.index.get_level_values(0)[0]\n",
    "#                                 for dysesh in short_term_strategy:\n",
    "#                                     if (check_ID in dysesh) & (shortened_sesh in dysesh) & ('PUPIL' not in dysesh):\n",
    "#                                         dynamic_seshes.append(sesh)\n",
    "\n",
    "#                             uncorrelated_dfs = []\n",
    "#                             for sesh in uncorrelated_df.index.get_level_values('Unique session').unique():\n",
    "#                                 sesh_df = sessions.get_group(sesh)\n",
    "#                                 if sesh in dynamic_seshes:\n",
    "#                                     None\n",
    "#                                 else:\n",
    "#                                     uncorrelated_dfs.append(sesh_df)\n",
    "#                             uncorrelated_df = pd.concat(uncorrelated_dfs)\n",
    "#                         else:\n",
    "#                             uncorrelated_df = uncorrelated_df                         \n",
    "                        \n",
    "                        outcome_dfs = []\n",
    "                        for index, rows in uncorrelated_df.iterrows():\n",
    "                            mouse = index[0]\n",
    "                            ROI = index[19]\n",
    "                            \n",
    "                            if len(correlated_ROIs[mouse].keys()) > 0:\n",
    "                                if ROI in correlated_ROIs[mouse].keys():\n",
    "                                    outcome_dfs.append(rows.to_frame().T)\n",
    "                        \n",
    "                        if len(outcome_dfs) == 0:\n",
    "                            None\n",
    "                        else:\n",
    "                            outcome_df = pd.concat(outcome_dfs)\n",
    "                            outcome_df.index.names = uncorrelated_df.index.names\n",
    "\n",
    "                            if trials == 'events':\n",
    "                                outcome_df = get_event_trials(outcome_df, 'deltaF', event_epochs)\n",
    "                            else:\n",
    "                                outcome_df = outcome_df\n",
    "\n",
    "                            mouse_counts = {}\n",
    "\n",
    "                            for mouse in outcome_df.index.get_level_values(0).unique():\n",
    "                                mouse_counts[mouse] = len(outcome_df[outcome_df.index.get_level_values(0) == mouse].index.get_level_values('Unique_ROI').unique())\n",
    "\n",
    "                            nmice = len(outcome_df.index.get_level_values(0).unique())\n",
    "                            nsessions = len(outcome_df.index.get_level_values('Unique session').unique())\n",
    "\n",
    "                            unique = outcome_df.groupby(outcome_df.index.get_level_values('Unique_ROI')).mean()\n",
    "                            nROIs = unique.shape[0]\n",
    "                            mean = unique.mean()\n",
    "                            sem = unique.sem()\n",
    "                            ax[col].plot(mean, color=color)\n",
    "                            ax[col].fill_between(x=list(range(unique.shape[1])), y1=mean-sem, y2=mean+sem, color=color, alpha=0.5)\n",
    "\n",
    "                            if epoch == 'all':\n",
    "                                ax[col].set_xticks(np.linspace(0,unique.shape[1],13))\n",
    "                                ax[col].set_xticklabels([int(x) for x in np.linspace(0,12,13)])      \n",
    "                            else:\n",
    "                                ax[col].set_xticks(np.linspace(0,unique.shape[1],11))\n",
    "                                ax[col].set_xticklabels([str(round(x,1)) for x in np.linspace(0,1,11)])\n",
    "                            title = str(nsessions)+' sessions/'+str(nROIs)+' ROIs \\n '\n",
    "\n",
    "                            for i, (key, value) in enumerate(mouse_counts.items()):\n",
    "                                title += ' \\n '+key+': '+str(value)+' ROIs'\n",
    "                            ax[col].set_title(title)\n",
    "                            ax[col].legend([label])                    \n",
    "                if trials == 'all':\n",
    "                    if plottype == 'ROIs':\n",
    "                        plt.suptitle(protocol+' '+epoch+' epoch ('+channel+') using all trials and all ROIs', fontsize=25, y=1.5)\n",
    "                    elif plottype == 'correlated_ROIs':\n",
    "                        plt.suptitle(protocol+' '+epoch+' epoch ('+channel+') using all trials and correlated ROIs only', fontsize=25, y=1.5)                        \n",
    "                else:\n",
    "                    if plottype == 'ROIs':\n",
    "                        plt.suptitle(protocol+' '+epoch+' epoch ('+channel+') using only trials with '+event_epochs+' events and all ROIs', fontsize=25, y=1.5)\n",
    "                    elif plottype == 'correlated_ROIs':\n",
    "                        plt.suptitle(protocol+' '+epoch+' epoch ('+channel+') using only trials with '+event_epochs+' events and correlated ROIs only', fontsize=25, y=1.5)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_trials(data, dtype, epoch):\n",
    "    events = pd.concat(cohort_epochs['events'][epoch])\n",
    "    events = events.droplevel(1)\n",
    "    \n",
    "    if dtype == 'pupil':\n",
    "        events['Date'] = [int(x) for x in list(events['Date'])]\n",
    "        IDs = pd.DataFrame([events.index], index=['mouse_ID']).T\n",
    "        midx = events.iloc[:,0:19].reset_index().iloc[:,1:]\n",
    "        final_outcomes = events['Final Outcome'].to_frame().reset_index().iloc[:,1:]\n",
    "        new_midx = pd.concat([IDs, midx, final_outcomes], axis=1)\n",
    "        events.index = pd.MultiIndex.from_tuples(tuple(np.asarray(new_midx)))\n",
    "        events = events.droplevel([6,19])\n",
    "    else:      \n",
    "        IDs = pd.DataFrame([events.index], index=['mouse_ID']).T\n",
    "        midx = events.iloc[:,0:19].reset_index().iloc[:,1:]\n",
    "        final_outcomes = events['Final Outcome'].to_frame().reset_index().iloc[:,1:]\n",
    "        new_midx = pd.concat([IDs, midx, final_outcomes], axis=1)\n",
    "        events.index = pd.MultiIndex.from_tuples(tuple(np.asarray(new_midx)))\n",
    "    events.index.names = pd.concat(cohort_epochs[dtype]['reward']).index.names \n",
    "    idx = data.index.intersection(events.index)\n",
    "    event_trials = data.loc[idx]\n",
    "    \n",
    "    return event_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04166fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_extract_correlated_ROIs(cohort_correlations, cohort_events, cohort_deltaF, r_thresh, event_thresh):\n",
    "    fname = 'correlated_ROIs_'+str(r_thresh)+'rthresh_'+str(event_thresh)+'eventthresh'\n",
    "    \n",
    "    if fname+'.pdf' in os.listdir(cwd):\n",
    "        print(fname+'.pdf already exists')\n",
    "    else:\n",
    "        pdf = matplotlib.backends.backend_pdf.PdfPages(fname+'.pdf')\n",
    "        correlated_ROIs = {}\n",
    "        for mouse, mouse_dict in cohort_correlations.items():\n",
    "            correlated_ROIs[mouse] = {}\n",
    "            if 'cross-pairs' in mouse_dict.keys():\n",
    "                for session, session_dict in mouse_dict['cross-pairs'].items():\n",
    "                    session_split = session.split('_')\n",
    "                    FPS = cohort_params[mouse][session_split[0]][session_split[1]][session_split[2]]['FPS']\n",
    "                    for ROI, ROI_dict in session_dict.items():\n",
    "\n",
    "                        mouse_df = cohort_deltaF[mouse]\n",
    "                        ROI_df = mouse_df[mouse_df.index.get_level_values('Unique_ROI') == ROI]\n",
    "                        trials = ROI_df.index.get_level_values('Trial')\n",
    "                        ROI_df = (ROI_df - ROI_df.min().min()) / (ROI_df.max().max() - ROI_df.min().min())\n",
    "                        ROI_flat = ROI_df.to_numpy().flatten()\n",
    "\n",
    "\n",
    "                        mouse_events = cohort_events[mouse]\n",
    "                        ROI_events = mouse_events[mouse_events['Unique_ROI'] == ROI]\n",
    "\n",
    "                        ROI_peak_times = []\n",
    "                        for index, rows in ROI_events.iterrows():\n",
    "                            trial_idx = np.where(np.asarray(trials) == rows.Trial)[0][0]\n",
    "                            peak_time = (trial_idx*ROI_df.shape[1]) + (rows['peak_time']*FPS)\n",
    "                            ROI_peak_times.append(peak_time) if peak_time < ROI_flat.shape[0] else None\n",
    "                        ROI_peaks = [ROI_flat[math.ceil(x)] for x in ROI_peak_times]\n",
    "\n",
    "                        ROI_color = 'green' if 'Green' in ROI else 'red'\n",
    "                        ROI_label = ('_').join(ROI.split('_')[-2:])\n",
    "                        for partner, r in ROI_dict.items():\n",
    "                            if r > r_thresh:\n",
    "                                partner_df = mouse_df[mouse_df.index.get_level_values('Unique_ROI') == partner]\n",
    "                                partner_df = (partner_df - partner_df.min().min()) / (partner_df.max().max() - partner_df.min().min())\n",
    "                                partner_flat = partner_df.to_numpy().flatten()\n",
    "                                partner_flat = partner_flat-0.75\n",
    "\n",
    "                                partner_events = mouse_events[mouse_events['Unique_ROI'] == partner]\n",
    "                                partner_peak_times = []\n",
    "                                for index, rows in partner_events.iterrows():\n",
    "                                    trial_idx = np.where(np.asarray(trials) == rows.Trial)[0][0]\n",
    "                                    peak_time = (trial_idx*partner_df.shape[1]) + (rows['peak_time']*FPS)\n",
    "                                    partner_peak_times.append(peak_time) if peak_time < partner_flat.shape[0] else None\n",
    "                                partner_peaks = [partner_flat[math.ceil(x)] for x in partner_peak_times]\n",
    "\n",
    "                                partner_color = 'green' if 'Green' in partner else 'red'\n",
    "                                partner_label = ('_').join(partner.split('_')[-2:])\n",
    "\n",
    "                                matched_event_times = []\n",
    "                                matched_event_peaks = []\n",
    "                                for ROI_time, ROI_peak in zip(ROI_peak_times, ROI_peaks):\n",
    "                                    for partner_time in partner_peak_times:\n",
    "                                        if distance(ROI_time, partner_time) < 15:\n",
    "                                            matched_event_times.append(ROI_time)\n",
    "                                            matched_event_peaks.append(ROI_peak+0.1)\n",
    "                                            \n",
    "                                event_ratio = len(matched_event_times) / len(ROI_peak_times)\n",
    "                                if event_ratio > event_thresh:\n",
    "                                    if ROI not in correlated_ROIs[mouse].keys():\n",
    "                                        correlated_ROIs[mouse][ROI] = {}\n",
    "                                        correlated_ROIs[mouse][ROI][partner] = {'r':r, 'event_ratio':event_ratio}\n",
    "                                    else:\n",
    "                                        correlated_ROIs[mouse][ROI][partner] = {'r':r, 'event_ratio':event_ratio}\n",
    "                                   \n",
    "                                    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(20,4))\n",
    "                                    fig.suptitle(mouse+'_'+session+' \\n '+ROI_label, fontsize=25, y=0.98)\n",
    "                                    ax.plot(partner_flat, color=partner_color, alpha=0.3)\n",
    "                                    ax.plot(ROI_flat, color=ROI_color,alpha=0.3)\n",
    "\n",
    "                                    ax.scatter(y=ROI_peaks, x=ROI_peak_times, color=ROI_color, marker='*', s=15)\n",
    "                                    ax.scatter(y=partner_peaks, x=partner_peak_times, color=partner_color, marker='*', s=15)\n",
    "                                    ax.scatter(y=matched_event_peaks, x=matched_event_times, color='orange', marker='*', s=25)\n",
    "\n",
    "                                    ax.set_title(partner_label, fontsize=15)  \n",
    "                                    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                                    pdf.savefig(fig)\n",
    "        np.save(fname+'.npy', correlated_ROIs)\n",
    "        pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d54c5",
   "metadata": {},
   "source": [
    "## Time to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3789cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['Ndis', 'Nrev', 'Edis', 'Erev'], outcomes=['hit','FA'], plottype='correlated_ROIs', trials='all', event_epochs='reward')\n",
    "# plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['Ndis','Edis'], outcomes=['hit','FA'], plottype='ROIs', trials='all', event_epochs='reward')\n",
    "plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['N2','E2'], outcomes=['rewarded','unrewarded'], plottype='ROIs', trials='all', event_epochs='reward')\n",
    "plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['N2','E2'], outcomes=['rewarded','unrewarded'], plottype='correlated_ROIs', trials='all', event_epochs='reward')\n",
    "\n",
    "# plot_calcium(cohort_epochs, channel='Green', epochs=['all'] ,protocols=['N','E'], outcomes=['hit','FA'], plottype='ROIs', trials='all', event_epochs='evnts')\n",
    "# plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['N'], outcomes=['hit','FA'], plottype='ROIs', trials='events', event_epochs='ITI')\n",
    "# plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['N'], outcomes=['hit','FA'], plottype='correlated_ROIs', trials='all', event_epochs='ITI')\n",
    "# plot_pupil(cohort_epochs, epochs=['ITI'] ,protocols=['Ndet','Edet','Ndis','Edis','Nrev','Erev'], outcomes=['hit','FA'], plottype='trials', trials='all', event_epochs='reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978e289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = pd.concat(cohort_epochs['deltaF']['all'])\n",
    "outcome_dfs = []\n",
    "sessions = outcome_df.groupby(outcome_df.index.get_level_values('Unique session'))\n",
    "for sesh in outcome_df.index.get_level_values('Unique session').unique():\n",
    "    shortened_sesh = sesh.split('FOV')[0][:-1]\n",
    "    sesh_df = sessions.get_group(sesh)\n",
    "    check_ID = sesh_df.index.get_level_values(0)[0]\n",
    "    for dysesh in short_term_strategy:\n",
    "        if (check_ID in dysesh) & (shortened_sesh in dysesh) & ('PUPIL' not in dysesh):\n",
    "            outcome_dfs.append(sesh_df)\n",
    "test = pd.concat(outcome_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['N'], outcomes=['hit','FA'], plottype='ROIs', trials='all', event_epochs='reward')\n",
    "# plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['N'], outcomes=['hit','FA'], plottype='ROIs', trials='events', event_epochs='reward')\n",
    "# plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['N'], outcomes=['hit','FA'], plottype='correlated_ROIs', trials='all', event_epochs='reward')\n",
    "plot_pupil(cohort_epochs, epochs=['all'] ,protocols=['Ndis','Nrev'], outcomes=['hit','FA'], plottype='trials', trials='all', event_epochs='reward')\n",
    "plot_pupil(cohort_epochs, epochs=['all'] ,protocols=['Edis','Erev'], outcomes=['hit','FA'], plottype='trials', trials='all', event_epochs='reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29960e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = outcome_df.groupby([outcome_df.index.get_level_values(0), outcome_df.index.get_level_values('Unique session')]).mean()\n",
    "test[test.index.get_level_values(0) == 'RGECO_GCamP_Batch3_one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7274c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['E'], outcomes=['hit','FA'], plottype='ROIs', trials='all', event_epochs='reward')\n",
    "plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['E'], outcomes=['hit','FA'], plottype='ROIs', trials='events', event_epochs='reward')\n",
    "plot_calcium(cohort_epochs, channel='Red', epochs=['all'] ,protocols=['E'], outcomes=['hit','FA'], plottype='correlated_ROIs', trials='all', event_epochs='reward')\n",
    "plot_pupil(cohort_epochs, epochs=['all'] ,protocols=['E'], outcomes=['hit','FA'], plottype='trials', trials='all', event_epochs='reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mouse, mouse_dict in deltaF_pupil.items():\n",
    "    if mouse not in somas:\n",
    "        for ROI, ROI_dict in mouse_dict.items():\n",
    "            deltaF = pd.DataFrame(ROI_dict['deltaF'])\n",
    "            deltaF = (deltaF - deltaF.mean())/deltaF.std(ddof=0)\n",
    "            pupil = pd.DataFrame(ROI_dict['pupil'])\n",
    "            pupil = (pupil - pupil.mean())/pupil.std(ddof=0)\n",
    "            \n",
    "            df = pd.concat([deltaF, pupil], axis=1)\n",
    "            rho = df.corr()\n",
    "            r = rho['deltaF']['pupil']\n",
    "            pval = (df.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape))['deltaF']['pupil']\n",
    "            \n",
    "            if (pval < 0.05) & (abs(r > 0.15)):\n",
    "                fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,5))\n",
    "                deltaF.plot(ax=ax, color='orange',alpha=0.6)\n",
    "                pupil.plot(ax=ax, color='blue',alpha=0.6)\n",
    "#             break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c24f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_ROIs = []\n",
    "for mouse, mouse_dict in cohort_correlations.items():\n",
    "    inter = mouse_dict['inter-compartment']\n",
    "    \n",
    "    for index, rows in inter.iterrows():\n",
    "        ID = mouse+'_'+index\n",
    "        if (rows['avg_r'] > 0.8):\n",
    "            mouse_deltaF = cohort_deltaF[mouse]\n",
    "            sesh = ('_').join(index.split('_')[:-2])\n",
    "            FOV_deltaF = mouse_deltaF[mouse_deltaF.index.get_level_values('Unique session') == sesh]\n",
    "            ROI_deltaF = FOV_deltaF[FOV_deltaF.index.get_level_values('Unique_ROI') == index].iloc[:,2:-1]\n",
    "            if 'Green' in index:\n",
    "                other_pop = FOV_deltaF[FOV_deltaF.index.get_level_values('Channel') == 'Red'].iloc[:,2:-1]\n",
    "                ROI_c = 'green'\n",
    "                other_c = 'red'\n",
    "            else:\n",
    "                other_pop = FOV_deltaF[FOV_deltaF.index.get_level_values('Channel') == 'Green'].iloc[:,2:-1]\n",
    "                ROI_c = 'red'\n",
    "                other_c = 'green'\n",
    "            ROI_flat = ROI_deltaF.to_numpy().flatten()\n",
    "            ROI_flat = (ROI_flat - ROI_flat.min())/(ROI_flat.max()-ROI_flat.min())\n",
    "            other_flat = other_pop.groupby(other_pop.index.get_level_values('Unique_trial')).mean().to_numpy().flatten()\n",
    "            other_flat = (other_flat - other_flat.min())/(other_flat.max()-other_flat.min())\n",
    "            plt.figure(figsize=(15,4))\n",
    "            plt.plot(ROI_flat, color=ROI_c, alpha=0.7)\n",
    "            plt.plot(other_flat, color=other_c, alpha=0.3)\n",
    "            plt.suptitle(mouse+'_'+index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad8d5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This application is used to convert notebook files (*.ipynb)\n",
      "        to various other formats.\n",
      "\n",
      "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "=======\n",
      "The options below are convenience aliases to configurable class-options,\n",
      "as listed in the \"Equivalent to\" description-line of the aliases.\n",
      "To see all configurable class-options for some <cmd>, use:\n",
      "    <cmd> --help-all\n",
      "\n",
      "--debug\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "    Equivalent to: [--Application.log_level=10]\n",
      "--show-config\n",
      "    Show the application's configuration (human-readable format)\n",
      "    Equivalent to: [--Application.show_config=True]\n",
      "--show-config-json\n",
      "    Show the application's configuration (json format)\n",
      "    Equivalent to: [--Application.show_config_json=True]\n",
      "--generate-config\n",
      "    generate default config file\n",
      "    Equivalent to: [--JupyterApp.generate_config=True]\n",
      "-y\n",
      "    Answer yes to any questions instead of prompting.\n",
      "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
      "--execute\n",
      "    Execute the notebook prior to export.\n",
      "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
      "--allow-errors\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
      "--stdin\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
      "--stdout\n",
      "    Write notebook output to stdout instead of files.\n",
      "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
      "--inplace\n",
      "    Run nbconvert in place, overwriting the existing notebook (only\n",
      "            relevant when converting to notebook format)\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
      "--clear-output\n",
      "    Clear output of current file and save in place,\n",
      "            overwriting the existing notebook.\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
      "--no-prompt\n",
      "    Exclude input and output prompts from converted document.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
      "--no-input\n",
      "    Exclude input cells and output prompts from converted document.\n",
      "            This mode is ideal for generating code-free reports.\n",
      "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
      "--allow-chromium-download\n",
      "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
      "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
      "--disable-chromium-sandbox\n",
      "    Disable chromium security sandbox when converting to PDF..\n",
      "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
      "--show-input\n",
      "    Shows code input. This flag is only useful for dejavu users.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
      "--embed-images\n",
      "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
      "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
      "--sanitize-html\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
      "--log-level=<Enum>\n",
      "    Set the log level by value or name.\n",
      "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
      "    Default: 30\n",
      "    Equivalent to: [--Application.log_level]\n",
      "--config=<Unicode>\n",
      "    Full path of a config file.\n",
      "    Default: ''\n",
      "    Equivalent to: [--JupyterApp.config_file]\n",
      "--to=<Unicode>\n",
      "    The export format to be used, either one of the built-in formats\n",
      "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf']\n",
      "            or a dotted object name that represents the import path for an\n",
      "            ``Exporter`` class\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.export_format]\n",
      "--template=<Unicode>\n",
      "    Name of the template to use\n",
      "    Default: ''\n",
      "    Equivalent to: [--TemplateExporter.template_name]\n",
      "--template-file=<Unicode>\n",
      "    Name of the template file to use\n",
      "    Default: None\n",
      "    Equivalent to: [--TemplateExporter.template_file]\n",
      "--theme=<Unicode>\n",
      "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
      "    as prebuilt extension for the lab template)\n",
      "    Default: 'light'\n",
      "    Equivalent to: [--HTMLExporter.theme]\n",
      "--sanitize_html=<Bool>\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
      "    should be set to True by nbviewer or similar tools.\n",
      "    Default: False\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
      "--writer=<DottedObjectName>\n",
      "    Writer class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: 'FilesWriter'\n",
      "    Equivalent to: [--NbConvertApp.writer_class]\n",
      "--post=<DottedOrNone>\n",
      "    PostProcessor class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
      "--output=<Unicode>\n",
      "    overwrite base name use for output files.\n",
      "                can only be used when converting one notebook at a time.\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.output_base]\n",
      "--output-dir=<Unicode>\n",
      "    Directory to write output(s) to. Defaults\n",
      "                                  to output to the directory of each notebook. To recover\n",
      "                                  previous default behaviour (outputting to the current\n",
      "                                  working directory) use . as the flag value.\n",
      "    Default: ''\n",
      "    Equivalent to: [--FilesWriter.build_directory]\n",
      "--reveal-prefix=<Unicode>\n",
      "    The URL prefix for reveal.js (version 3.x).\n",
      "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
      "            of reveal.js.\n",
      "            For speaker notes to work, this must be a relative path to a local\n",
      "            copy of reveal.js: e.g., \"reveal.js\".\n",
      "            If a relative path is given, it must be a subdirectory of the\n",
      "            current directory (from which the server is run).\n",
      "            See the usage documentation\n",
      "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
      "            for more details.\n",
      "    Default: ''\n",
      "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
      "--nbformat=<Enum>\n",
      "    The nbformat version to write.\n",
      "            Use this to downgrade notebooks.\n",
      "    Choices: any of [1, 2, 3, 4]\n",
      "    Default: 4\n",
      "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to html\n",
      "\n",
      "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf'].\n",
      "\n",
      "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "\n",
      "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
      "            'classic'. You can specify the flavor of the format used.\n",
      "\n",
      "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
      "\n",
      "            You can also pipe the output to stdout, rather than a file\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "\n",
      "            PDF is generated via latex\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "\n",
      "            You can get (and serve) a Reveal.js-powered slideshow\n",
      "\n",
      "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "\n",
      "            Multiple notebooks can be given at the command line in a couple of\n",
      "            different ways:\n",
      "\n",
      "            > jupyter nbconvert notebook*.ipynb\n",
      "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "\n",
      "            or you can specify the notebooks list in a config file, containing::\n",
      "\n",
      "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "\n",
      "            > jupyter nbconvert --config mycfg.py\n",
      "\n",
      "To see all available configurables, use `--help-all`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern '{notebook_path}' matched no files\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script {notebook_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182a1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
